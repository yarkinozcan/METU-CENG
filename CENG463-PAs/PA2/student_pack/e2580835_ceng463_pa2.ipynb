{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c79918-eec4-4e96-99ab-673c15287380",
   "metadata": {},
   "source": [
    "# CENG463 PA2\n",
    "\n",
    "In this programming assignment, you will be dealing with word embeddings and neural networks. You will use Python for this task. You can use libraries such as `pandas`, `nltk`, `numpy` etc. for your implementations, or implement your own functions. However, you are expected to analyse and reason about your implementation and results. The assignment consists of 3 questions.\n",
    "\n",
    "### IMPORTANT NOTE\n",
    "\n",
    "Do not move or delete the given cells, only add cells inbetween the questions for your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d352749-cf05-4610-b0c1-f6670d538f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# UPDATE THIS CELL TO INSTALL NEEDED LIBRARIES.\n",
    "# MAKE SURE TO ADD EVERYTHING THAT NEEDS TO BE INSTALLED IN THIS CELL!\n",
    "\n",
    "# we will use pip to install packages - you can add others below\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install scikit-learn\n",
    "\n",
    "# and import them here - you can add others below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    ")\n",
    "import time\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff1907-973f-4895-bd43-0a30c7e0a029",
   "metadata": {},
   "source": [
    "## Q1 - Word embeddings (50 points)\n",
    "\n",
    "In this question, you will first train a Word2Vec model, then use it to represent and reason about user reviews.\n",
    "\n",
    "### Q1.A - training (10 points)\n",
    "Load the `user_review_train.csv` file shared with you. Using `Word2Vec` module of `gensim.models`, train a **skip-gram** Word2Vec model on the train data.\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- Use the given preprocessing function `preprocess_review`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "980ad5b5-de0a-4ba0-ad1d-5a45a9543d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yarkinozcan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yarkinozcan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yarkinozcan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING FUNCTIONS GIVEN FOR YOU\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def preprocess_review(review):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = sent_tokenize(review)\n",
    "    \n",
    "    lemmatized_review = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        lowercased_sentence = [token.lower() for token in tokenized_sentence]\n",
    "        stopwords_removed_sentence = [token for token in lowercased_sentence if token not in stop_words]\n",
    "        lemmatized_sentence = [lemmatizer.lemmatize(token) for token in stopwords_removed_sentence]\n",
    "        \n",
    "        lemmatized_review = lemmatized_review + lemmatized_sentence\n",
    "    \n",
    "    return lemmatized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "73788893-4f65-4821-a34c-57ac85e4698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# Q1.A - implementation\n",
    "# you can add cells below if needed\n",
    "\n",
    "# Load the training data\n",
    "train = pd.read_csv(\"data/user_review_train.csv\") # review are on the 'review' column\n",
    "\n",
    "# List for storing the tokenized reviews\n",
    "tokenized_reviews = []\n",
    "\n",
    "for review in train['review']:\n",
    "    # preprocess each review using given function\n",
    "    tokens = preprocess_review(review) # This list stores tokens for single review\n",
    "    tokenized_reviews.append(tokens) # This list stores tokens for all reviews (a list of lists)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_reviews, # A list of list where each list includes tokens for each review\n",
    "    vector_size=100, # The number of dimensions in each embedding vector\n",
    "    window=5, # The sliding window size -> this is for checking the context of a word\n",
    "    min_count=3, # words occuring less than 1 times are ignored\n",
    "    workers=4, # This is the number of cores that will be used in training -> to accelerate training \n",
    "    sg=1, # This is for enabling skip-gram\n",
    "    epochs=5, # The number of times model goes through the dataset -> I tried increasing this but it made all similirity scores go really low possibly causing an overfit\n",
    ")\n",
    "\n",
    "print('training complete') # I added this to track if the training is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fed93-b026-4ccb-931c-b2afc1f9e9ea",
   "metadata": {},
   "source": [
    "### Q1.B - word similarity (10 points)\n",
    "\n",
    "Using the trained model, report the following:\n",
    "\n",
    "- Similarity between \"good\" and \"bad\"\n",
    "- Similar words to \"good\"\n",
    "- Similar words to \"bad\"\n",
    "- Similar words to \"good\" but not similar to \"bad\"\n",
    "- Similar words to \"good\" but not similar to \"bad\"\n",
    "\n",
    "and discuss the reported words and scores. Is it possible to identify specific good/bad features of the product that is being reviewed? What other words can be looked up to get more insight?\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- Check the [documentation](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html) of `gensim.models.Word2Vec` to find relevant methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "997de6f5-d160-4a2a-a835-89bd13e5ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between \"good\" and \"bad\": 0.7342\n",
      "\n",
      "Top similar words to 'good':\n",
      "  nice             0.9153\n",
      "  superb           0.9068\n",
      "  awesome          0.9057\n",
      "  excellent        0.9023\n",
      "  beautiful        0.8899\n",
      "  okay             0.8854\n",
      "  gud              0.8829\n",
      "  perfect          0.8781\n",
      "  fantastic        0.8688\n",
      "  decent           0.8682\n",
      "\n",
      " \n",
      "\n",
      "Top similar words to 'bad':\n",
      "  poor             0.8454\n",
      "  pathetic         0.7778\n",
      "  worst            0.7687\n",
      "  bed              0.7643\n",
      "  nd               0.7576\n",
      "  wrost            0.7541\n",
      "  ........         0.7528\n",
      "  .......          0.7516\n",
      "  satisfactory     0.7510\n",
      "  sad              0.7477\n",
      "\n",
      " \n",
      "\n",
      "Words similar to 'good' but not similar to 'bad':\n",
      "  wise             0.4420\n",
      "  look             0.3945\n",
      "  specification    0.3918\n",
      "  range            0.3879\n",
      "  cool             0.3796\n",
      "  quite            0.3791\n",
      "  nice             0.3742\n",
      "  price            0.3708\n",
      "  premium          0.3641\n",
      "  feature          0.3595\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Words similar to 'bad' but not similar to 'good':\n",
      "  customer         0.3791\n",
      "  care             0.3646\n",
      "  center           0.3329\n",
      "  told             0.3159\n",
      "  reply            0.3137\n",
      "  visit            0.2959\n",
      "  refund           0.2958\n",
      "  policy           0.2873\n",
      "  tried            0.2850\n",
      "  request          0.2845\n"
     ]
    }
   ],
   "source": [
    "# Q1.B - implementation\n",
    "# you can add cells below if needed\n",
    "\n",
    "# Check the similarity score between 'good' and 'bad'\n",
    "# Here in order to reach word vectors I had to use .wv\n",
    "similarity_score = model.wv.similarity('good', 'bad') # Found a .similarity(w1, w2) method in the documentation\n",
    "print(f'Similarity between \"good\" and \"bad\": {similarity_score:.4f}\\n')\n",
    "\n",
    "# Check the mostSimilar words to 'good'\n",
    "print(\"Top similar words to 'good':\") \n",
    "# Found a .most_similar method in the documentation\n",
    "# I kept topn parameter as default (value is 10)\n",
    "# The return value is a 2-tuple (word-score pairs)\n",
    "most_similar_good = model.wv.most_similar(positive=[\"good\"]) \n",
    "for word, score in most_similar_good:\n",
    "    print(f\"  {word:15s}  {score:.4f}\")\n",
    "\n",
    "print('\\n \\n') # In order to clearly seperate results\n",
    "\n",
    "# Check the mostSimilar words to 'bad'\n",
    "print(\"Top similar words to 'bad':\") \n",
    "# Again the same things\n",
    "most_similar_bad = model.wv.most_similar(positive=[\"bad\"]) \n",
    "for word, score in most_similar_bad:\n",
    "    print(f\"  {word:15s}  {score:.4f}\")\n",
    "\n",
    "print('\\n \\n') # In order to clearly seperate results\n",
    "\n",
    "# Check the words similar to 'good' but not similar to 'bad'\n",
    "print(\"Words similar to 'good' but not similar to 'bad':\")\n",
    "#Here the input negative is for words not similar to a related words\n",
    "good_not_bad = model.wv.most_similar(positive=[\"good\"], negative=[\"bad\"])\n",
    "for word, score in good_not_bad:\n",
    "    print(f\"  {word:15s}  {score:.4f}\")\n",
    "print()\n",
    "\n",
    "print('\\n \\n') # In order to clearly seperate results\n",
    "\n",
    "# Check the words similar to 'bad' but not similar to 'good'\n",
    "print(\"Words similar to 'bad' but not similar to 'good':\")\n",
    "bad_not_good = model.wv.most_similar(positive=[\"bad\"], negative=[\"good\"])\n",
    "for word, score in bad_not_good:\n",
    "    print(f\"  {word:15s}  {score:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b83305-d952-4d88-bc46-0dadd68f26a9",
   "metadata": {},
   "source": [
    "### Q1.B - discussion\n",
    "\n",
    "**1. Similarity between \"good\" and \"bad\"**\n",
    "The similarity score is high, which is expected in Word2Vec models. Although \"good\" and \"bad\" are antonyms, they share the same syntactic context. They appear in identical sentence structures (e.g., \"The battery is good\" vs. \"The battery is bad\"), causing their vectors to be close in the embedding space. We can also infer that word2vec word embeddings DOES NOT give information about synonyms and antonyms. It gives information about similarity.\n",
    "\n",
    "**2. Discussion of Similar Words**\n",
    "* **Neighbors of \"good\":** The model successfully clusters positive adjectives like 'nice', 'superb', and 'awesome'. It also captures internet slang such as 'gud' (good), reflecting the informal nature of the dataset. Also it is important that the neighbours 'good' have really high similarity to 'good'. This will be useful later.\n",
    "* **Neighbors of \"bad\":** The list includes strong negative adjectives like 'poor' and 'pathetic'. Also compared 'good' and its neighbours, the similarity between the word 'bad' and its neighbours are quite low. This means that the model confidently identify words similar to 'good' instead of 'bad'.\n",
    "\n",
    "**3. Vector Arithmetic (Isolating Sentiment)**\n",
    "* **Good - Bad (Pure Positivity):** Subtracting the 'bad' vector highlights words related to positivty rather than just general praise. Words like 'wise', and 'price' appear, suggesting that a key driver for 'good' reviews is value for money. Also we get words like 'nice', 'wise' or 'impressive' which is expected since the they all indicate positive meaning.\n",
    "* **Bad - Good (Pure Negativity):** Subtracting the 'good' vector isolates unique negative experiences. Words like 'customer', 'care' and 'reply' appear here. This indicates that while 'bad' describes hardware, the uniquely negative experiences are often related to customer service (lack of reply). It is important to note that compare to 'good'-'bad', here we do not have words with negative meaning such as 'poor' or 'pathetic'. I believe this is due to the fact that model is more confident when predicting words similar to 'good' than to the word 'bad'.\n",
    "\n",
    "**4. Can we identify specific product features?**\n",
    "Yes, by examining the neighbors of 'bad', we can identify that Customer Care ('customer') and ('care') are the primary pain points for this product. Conversely, the neighbors of 'good' (minus 'bad') suggest that Price/Value ('budget', 'wise') is a specific positive feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c25cf0-926a-4536-b2e5-f0bdb1931672",
   "metadata": {},
   "source": [
    "### Q1.C - representation (15 points)\n",
    "\n",
    "An important use of word embeddings is representing \"documents\" (reviews in our case). For this question, before creating the representations, do the following:\n",
    "\n",
    "- Randomly sample 2 reviews from sentiment label 0, refer to them as sent0_a and sent0_b.\n",
    "- Randomly sample 2 reviews from sentiment label 1, refer to them as sent1_a and sent1_b.\n",
    "\n",
    "After the sampling, follow these steps to represent each review:\n",
    "\n",
    "- Preprocess the review with the given `preprocess_review` function.\n",
    "- For each token in the review, fetch the vector of that token.\n",
    "- Take the average of the token vectors in the review to represent that review.\n",
    "\n",
    "Then, calculate and report the cosine similarity of the two vectors representing:\n",
    "    - sent0_a and sent0_b\n",
    "    - sent0_a and sent1_a\n",
    "    - sent1_a and sent1_b\n",
    "\n",
    "Does this representation work to capture the labels of the reviews? Do you think there is a better way to represent each review instead of taking the average of the word vectors? Discuss your findings with respect to these questions. Repeating the sampling process several times might give you a better insight.\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- You can use `numpy` for your calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "38bf5723-ee84-46a0-b758-ecd0ad8d055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not buy this phone, got hanged and heated\n",
      "Camera is very disappointing and after updating to latest versions front camera loses the quality and music out was very poor as compared to k3note, k4 note and there is an option for slow motion video which is the worst feature in this phone\n",
      "best for under 12k\n",
      "Awsm product......\n",
      "\n",
      "\n",
      "cosine(sent0_a, sent0_b) = 0.8104\n",
      "cosine(sent0_a, sent1_a) = 0.7138\n",
      "cosine(sent1_a, sent1_b) = 0.7352\n"
     ]
    }
   ],
   "source": [
    "# Q1.C - implementation\n",
    "# you can add cells below if needed\n",
    "\n",
    "# Get the sentiment 0 reviews\n",
    "sent0 = train[train[\"sentiment\"] == 0] # This gives both sentiments and reviews\n",
    "sent0_samples = sent0.sample(2) # Randomly sample 2 reviews\n",
    "\n",
    "# Get the sentiment 1 reviews\n",
    "sent1 = train[train[\"sentiment\"] == 1] # This gives both sentiments and reviews\n",
    "# Here I did not set random_state=0 or 1 since each time I run this I want to get a different randomly selected pair\n",
    "sent1_samples = sent1.sample(2) # Randomly sample 2 reviews\n",
    "\n",
    "# Here when I used sent0_samples[0] I got an error. So when I looked it up the trick is to use iloc[0]\n",
    "sent0_a = sent0_samples.iloc[0][\"review\"]\n",
    "sent0_b = sent0_samples.iloc[1][\"review\"]\n",
    "sent1_a = sent1_samples.iloc[0][\"review\"]\n",
    "sent1_b = sent1_samples.iloc[1][\"review\"]\n",
    "\n",
    "print(sent0_a)\n",
    "print(sent0_b)\n",
    "print(sent1_a)\n",
    "print(sent1_b)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Helper function to get average embedding for a review\n",
    "def calculate_average_tokens(review_text, model):\n",
    "    tokens = preprocess_review(review_text)\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        # If no token has a vector (very rare), return a zero vector\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "# Helper function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(v1, v2) / (norm1 * norm2))\n",
    "\n",
    "\n",
    "# Compute review vectors using the helper method\n",
    "sent0_a_vec = calculate_average_tokens(sent0_a, model)\n",
    "sent0_b_vec = calculate_average_tokens(sent0_b, model)\n",
    "sent1_a_vec = calculate_average_tokens(sent1_a, model)\n",
    "sent1_b_vec = calculate_average_tokens(sent1_b, model)\n",
    "\n",
    "\n",
    "# 5) Compute requested cosine similarities\n",
    "sim_00 = cosine_similarity(sent0_a_vec, sent0_b_vec)  # 0 vs 0\n",
    "sim_01 = cosine_similarity(sent0_a_vec, sent1_a_vec)  # 0 vs 1\n",
    "sim_11 = cosine_similarity(sent1_a_vec, sent1_b_vec)  # 1 vs 1\n",
    "\n",
    "print(f\"cosine(sent0_a, sent0_b) = {sim_00:.4f}\")\n",
    "print(f\"cosine(sent0_a, sent1_a) = {sim_01:.4f}\")\n",
    "print(f\"cosine(sent1_a, sent1_b) = {sim_11:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed704b27-9a9e-4af1-8f98-6ee4aeaeab4e",
   "metadata": {},
   "source": [
    "### Q1.C - discussion\n",
    "\n",
    "**Does this representation work to capture the labels?**\n",
    "Based on the results, the simple averaging of word vectors is moderately effective but inconsistent.\n",
    "* **Expectation:** We expect `Sim(Neg, Neg)` and `Sim(Pos, Pos)` to be high, and `Sim(Neg, Pos)` to be low.\n",
    "* **Reality:** Often, the similarity between *any* two reviews is relatively high (e.g., > 0.60). This is because both positive and negative reviews share the same domain vocabulary (e.g., \"phone\", \"battery\", \"price\", \"delivery\"). When we average all vectors, these common nouns dominate the vector direction, washing out the sentiment-specific adjectives like \"good\" or \"bad\". Also the results are really inconsistent, for each different run, I get a very different result. For most of the results, similarity scores between same sentiment reviews are higher but for some cases the similarity between average vectors from different sentiment reviews are higher. This actually shows that there are better ways for representing documents than Word2vec model. It might be that Word2vec is strong for representing meaning of words but not for representing meanings for documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb65f33-6f79-44e8-8b68-c054ad7a7707",
   "metadata": {},
   "source": [
    "### Q1.D - training and comparing classifiers (15 points)\n",
    "\n",
    "For this task, you will use the `user_review_train.csv` and `user_review_test.csv` files to train a binary classification model with Word2Vec representations, and compare its performance with a binary classifier using Bag-of-Words representation.\n",
    "\n",
    "As the Bag-of-Words classifier, you can either choose the best performing classifier you have implemented in Question 3 of Programming Assignment 1, or you can follow these steps:\n",
    "\n",
    "- Preprocess the review with the given `preprocess_review` function.\n",
    "- Order all unique tokens by frequency, take the most frequent 100.\n",
    "- Use these 100 words as the corpus for Bag-of-Words representation.\n",
    "\n",
    "For the Word2Vec model, represent the reviews by following these steps:\n",
    "\n",
    "- Preprocess the review with the given `preprocess_review` function.\n",
    "- For each token in the review that is also in the most frequent 100 tokens, fetch the vector of that token.\n",
    "- Take the average of the token vectors selected to represent that review.\n",
    "\n",
    "After training both classifiers on `user_review_train.csv`, test them with `user_review_test.csv` and report the performance of your models with four metrics: accuracy, precision, recall and F1-score. Compare the performance of both models and discuss in detail.\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- You can use `CountVectorizer` from `scikit-learn` or any other library available for Bag-of-Words representation.\n",
    "- You should select a classification method from the following set of classifiers: `[Naive Bayes, Support Vector Machine, Logistic Regression, Random Forest]`. You can use `scikit-learn`, `nltk`, or any other library for the classifier implementations. \n",
    "- You should **not** use the test set `user_reviews_test.csv` during your training process. You should use `user_reviews_train.csv` only.\n",
    "- You may add a validation step in your training process. To do this, you can further split the `user_reviews_train.csv` data and apply k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56d6c1b4-271a-405d-8103-8be7fd29e496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bag-of-Words:\n",
      "Accuracy:  0.7731\n",
      "Precision: 0.8630\n",
      "Recall:    0.7196\n",
      "F1 Score:  0.7848\n",
      "Results for Word2Vec:\n",
      "Accuracy:  0.7600\n",
      "Precision: 0.8555\n",
      "Recall:    0.7009\n",
      "F1 Score:  0.7705\n"
     ]
    }
   ],
   "source": [
    "# Q1.D - implementation\n",
    "# you can add cells below if needed\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(\"data/user_review_test.csv\")\n",
    "\n",
    "# First preprocess each review\n",
    "# Create a tokens column for both train and test data\n",
    "train['tokens'] = train['review'].apply(preprocess_review)\n",
    "test['tokens'] = test['review'].apply(preprocess_review)\n",
    "\n",
    "# Get all words in a single list since we are going to use bag of words\n",
    "all_train_words = [word for tokens in train['tokens'] for word in tokens]\n",
    "word_counts = Counter(all_train_words) # this returns word, count pair\n",
    "top_100_tuples = word_counts.most_common(100) # extract the most common 100 words\n",
    "top_100_words = [word for word, count in top_100_tuples] # extract only the words from tuples\n",
    "\n",
    "\n",
    "# Initialize CountVectorizer with only the top 100 vocabulary\n",
    "vectorizer = CountVectorizer(vocabulary=top_100_words)\n",
    "\n",
    "# Prepare Features (X) and Sentiments (y)\n",
    "# Join tokens back to string because CountVectorizer expects strings\n",
    "train_corpus = [\" \".join(tokens) for tokens in train['tokens']]\n",
    "test_corpus = [\" \".join(tokens) for tokens in test['tokens']]\n",
    "\n",
    "# Transform text into vector of numbers (where each dimension of the vector is the most common tokens)\n",
    "X_train_bow = vectorizer.transform(train_corpus)\n",
    "X_test_bow = vectorizer.transform(test_corpus)\n",
    "\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']\n",
    "\n",
    "# Train Classifier (Logistic Regression)\n",
    "classifier_bow = LogisticRegression(random_state=42, max_iter=1000)\n",
    "classifier_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_bow = classifier_bow.predict(X_test_bow)\n",
    "\n",
    "# Metrics\n",
    "print(\"Results for Bag-of-Words:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_bow):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_bow):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_bow):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_bow):.4f}\")\n",
    "\n",
    "# Helper function to get w2v vectors that only uses the most common 100 tokens\n",
    "def get_filtered_w2v_vector(tokens, model, valid_vocab):\n",
    "    # Keep word only if it is in Top 100 and in Word2Vec model\n",
    "    valid_tokens = [word for word in tokens if word in valid_vocab and word in model.wv]\n",
    "    \n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Fetch vectors\n",
    "    vectors = [model.wv[word] for word in valid_tokens]\n",
    "    \n",
    "    # Return Average\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Generate Features\n",
    "X_train_w2v = np.array([get_filtered_w2v_vector(t, model, top_100_words) for t in train['tokens']])\n",
    "X_test_w2v = np.array([get_filtered_w2v_vector(t, model, top_100_words) for t in test['tokens']])\n",
    "\n",
    "# Train Classifier (Logistic Regression)\n",
    "classifier_w2v = LogisticRegression(random_state=42, max_iter=1000)\n",
    "classifier_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_w2v = clf_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Metrics\n",
    "print(\"Results for Word2Vec:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_w2v):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_w2v):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_w2v):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_w2v):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d10d6-77f6-4990-ad82-48254063584d",
   "metadata": {},
   "source": [
    "### Q1.D - discussion\n",
    "\n",
    "**Performance Summary**\n",
    "- Bag-of-Words (BoW): Accuracy 77.3% | F1 Score 0.78\n",
    "- Word2Vec (W2V): Accuracy 76.0% | F1 Score 0.77 \n",
    "\n",
    "**Observation:**\n",
    "The Bag-of-Words model outperformed the Word2Vec model across all reported metrics (Accuracy, Precision, and Recall), though the margin is relatively small.\n",
    "\n",
    "At first I thought that the advanced method (word2vec) would outperform the simpler one (bag of words), but given the constraints on the task, this behaviour is expected due to the fact that in our Word2Vec approach, we represented a review by taking the average of its word vectors. This causes a loss of information.\n",
    "- Example: If a review says \"good screen, bad battery\", the vector for \"good\" (positive direction) and \"bad\" (negative direction) might mathematically cancel each other out when averaged, resulting in a neutral vector.\n",
    "- BoW Advantage: Bag of words keeps these counts separate ('good':1, 'bad':1). The Logistic Regression can then learn that \"good\" contributes +X to the score and \"bad\" contributes -Y, preserving both signals.\n",
    "\n",
    "As I have guessed in part C, While Word2Vec is generally a more powerful representation for capturing semantics, Bag-of-Words is often superior for simple sentiment analysis tasks when using linear classifiers and restricted vocabularies. The explicit presence/absence of specific sentiment keywords (captured by BoW) is often a stronger signal than the average semantic direction (captured by averaged W2V)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71090a0-8158-46d6-a6d6-d90101eeb464",
   "metadata": {},
   "source": [
    "## Q2 - Neural Networks for Binary Classification (50 points)\n",
    "\n",
    "For this task, you will use the `user_review_train.csv` and `user_review_test.csv` files to train two neural network models for the binary classification of user reviews and compare their performances. You are expected to train RNN (part A - 20 points) and TextCNN (part B - 20 points) models, and report the following: \n",
    "\n",
    "- Confusion matrix of both models\n",
    "- Time it took to train both models\n",
    "- Accuracy, precision, recall, and F1-score of both models\n",
    "- Other metrics you think are important\n",
    "\n",
    "Finally (part C), you should discuss the performance of the models according to your reported results. Try to analyse the models in terms of pros and cons of using each one.\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- For the embedding layers of the models, you are free to use word embedding methods or leave them randomly initialised. Similarly, you can use word-based or character-based embeddings. However, make sure to explain your decisions.\n",
    "- You are expected to use `tensorflow` for your implementations, but you can use other libraries if you already have a working setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4e5472c-1f1e-4b78-bd3e-0a9014c29337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train padded shape: (11751, 100)\n",
      "Test padded shape : (1675, 100)\n",
      "Vocabulary size   : 10001\n"
     ]
    }
   ],
   "source": [
    "# Preparation step that is required for both Q2.A and Q2.B\n",
    "\n",
    "# Load the training and test datasets\n",
    "# The reason I read these inputs again is because I can run directly this cell withohut running previous ones\n",
    "train = pd.read_csv(\"data/user_review_train.csv\")\n",
    "test  = pd.read_csv(\"data/user_review_test.csv\")\n",
    "\n",
    "# Preprocess each review and get the tokens\n",
    "# Here train_tokens is a list of token lists\n",
    "train_tokens = [preprocess_review(str(r)) for r in train_df[\"review\"]]\n",
    "test_tokens  = [preprocess_review(str(r)) for r in test_df[\"review\"]]\n",
    "\n",
    "counter = Counter()\n",
    "for t in train_tokens:\n",
    "    counter.update(t)\n",
    "\n",
    "max_vocab = 10000    # Get the most common 10000 words in order to get rid of very uncommon words\n",
    "most_common = counter.most_common(max_vocab)\n",
    "\n",
    "# Here I am assigning each word an index so that I can directly get the embedding vector of that vector\n",
    "word_index = {w: i+1 for i, (w, c) in enumerate(most_common)}\n",
    "vocab_size = len(word_index) + 1 # Since indexing start from 0\n",
    "\n",
    "# Helper function to convert a list of words (tokens) into a list of integers (word indexes).\n",
    "def to_sequence(tokens):\n",
    "    return [word_index[w] for w in tokens if w in word_index]\n",
    "\n",
    "# Use this to store the reviews as a list of word indexes\n",
    "review_train_seq = [to_sequence(t) for t in train_tokens]\n",
    "review_test_seq  = [to_sequence(t) for t in test_tokens]\n",
    "\n",
    "maxlen = 100  # fixed-length sequence\n",
    "# For LSTM, the input size must be same for each review, so for shorter review, we do padding and for long reviews we truncate so that \n",
    "# we get a fixed size input\n",
    "review_train_pad = pad_sequences(review_train_seq, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
    "review_test_pad  = pad_sequences(review_test_seq,  maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(\"Train padded shape:\", review_train_pad.shape)\n",
    "print(\"Test padded shape :\", review_test_pad.shape)\n",
    "print(\"Vocabulary size   :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06b9679c-eaf5-4986-8110-6fe51df4162c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.8048 - loss: 0.4241 - val_accuracy: 0.7509 - val_loss: 0.5223\n",
      "Epoch 2/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8911 - loss: 0.2793 - val_accuracy: 0.7874 - val_loss: 0.4641\n",
      "Epoch 3/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 77ms/step - accuracy: 0.9087 - loss: 0.2381 - val_accuracy: 0.7517 - val_loss: 0.5719\n",
      "Epoch 4/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.9211 - loss: 0.2108 - val_accuracy: 0.7781 - val_loss: 0.5645\n",
      "Epoch 5/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 77ms/step - accuracy: 0.9349 - loss: 0.1800 - val_accuracy: 0.7636 - val_loss: 0.6645\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "RNN Training Time: 62.126697063446045\n",
      "Accuracy: 0.8370149253731344\n",
      "Precision: 0.8920454545454546\n",
      "Recall: 0.8151609553478713\n",
      "F1: 0.8518719479110146\n",
      "Confusion Matrix:\n",
      " [[617  95]\n",
      " [178 785]]\n"
     ]
    }
   ],
   "source": [
    "# Q2.A - implementation of RNN\n",
    "# you can add cells below if needed\n",
    "\n",
    "# The number of features in each word vector\n",
    "embedding_dim = 100 \n",
    "\n",
    "rnn_model = Sequential([ # # Sequential means each layer feeds directly into the next\n",
    "    Embedding(vocab_size, embedding_dim), # Converts integer-encoded words into dense embedding vectors\n",
    "    # Bidirectional LSTM to capture context from both left and right.\n",
    "    Bidirectional(LSTM(64)), # Use LSTM to keep the meaning of the previous words in the memory and \n",
    "    Dense(64, activation='relu'), # A small dense layer to learn non-linear combinations of LSTM features\n",
    "    Dense(1, activation=\"sigmoid\") # Produces a probability between 0 and 1 indicating sentiment\n",
    "])\n",
    "\n",
    "# metrics = accuracy -> track accuracy during training\n",
    "# user binary cross entropy for calculating the loss\n",
    "rnn_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "# Start the timer to calculate the training time of RNN\n",
    "start = time.time()\n",
    "\n",
    "# Train the model with padded input and sentiments\n",
    "# Here I used padded input for RNN aswell because the training done in batches\n",
    "rnn_model.fit(review_train_pad, sentiment_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Calculate the total training time\n",
    "rnn_time = time.time() - start\n",
    "\n",
    "# Make the model predict sentiments\n",
    "rnn_pred = (rnn_model.predict(review_test_pad) > 0.5)\n",
    "\n",
    "print(\"RNN Training Time:\", rnn_time)\n",
    "print(\"Accuracy:\",  accuracy_score(sentiment_test, rnn_pred))\n",
    "print(\"Precision:\", precision_score(sentiment_test, rnn_pred))\n",
    "print(\"Recall:\",    recall_score(sentiment_test, rnn_pred))\n",
    "print(\"F1:\",        f1_score(sentiment_test, rnn_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(sentiment_test, rnn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39f7b939-e486-4fa1-b053-53f2e851992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8000 - loss: 0.4289 - val_accuracy: 0.7491 - val_loss: 0.5721\n",
      "Epoch 2/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8944 - loss: 0.2701 - val_accuracy: 0.8010 - val_loss: 0.4798\n",
      "Epoch 3/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9262 - loss: 0.1966 - val_accuracy: 0.7696 - val_loss: 0.6458\n",
      "Epoch 4/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9545 - loss: 0.1354 - val_accuracy: 0.7747 - val_loss: 0.7046\n",
      "Epoch 5/5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9666 - loss: 0.0992 - val_accuracy: 0.7959 - val_loss: 0.6913\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "CNN Training Time: 11.416546821594238\n",
      "Accuracy: 0.8226865671641791\n",
      "Precision: 0.8588362068965517\n",
      "Recall: 0.8276220145379024\n",
      "F1: 0.8429402432575357\n",
      "Confusion Matrix:\n",
      " [[581 131]\n",
      " [166 797]]\n"
     ]
    }
   ],
   "source": [
    "# Q2.B - TextCNN model for sentiment classification\n",
    "\n",
    "# Build a simple TextCNN model using Sequential, where each layer feeds directly into the next\n",
    "cnn_model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim), # Converts integer-encoded words into dense embedding vectors\n",
    "    # Applies 128 filters that slide over the sequence of embeddings\n",
    "    # kernel_size=5 means each filter looks at a window of 5 consecutive words (like detecting a phrase)\n",
    "    Conv1D(filters=128, kernel_size=5, activation=\"relu\"),\n",
    "    # Takes the maximum value of each filter over the entire sequence\n",
    "    # Reduces the sequence to a fixed-size vector regardless of sentence length\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Same with RNN example\n",
    "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Start timer for measuring CNN training time\n",
    "start = time.time()\n",
    "\n",
    "# Train the TextCNN on padded sequences\n",
    "# the batches still require fixed-size tensors, so padding is still necessary\n",
    "cnn_model.fit(review_train_pad, sentiment_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Measure total training time for CNN\n",
    "cnn_time = time.time() - start\n",
    "\n",
    "# Make predictions on the test set\n",
    "# The model outputs probabilities, so we threshold > 0.5 to get binary predictions\n",
    "cnn_pred = (cnn_model.predict(review_test_pad) > 0.5)\n",
    "\n",
    "print(\"CNN Training Time:\", cnn_time)\n",
    "print(\"Accuracy:\",  accuracy_score(sentiment_test, cnn_pred))\n",
    "print(\"Precision:\", precision_score(sentiment_test, cnn_pred))\n",
    "print(\"Recall:\",    recall_score(sentiment_test, cnn_pred))\n",
    "print(\"F1:\",        f1_score(sentiment_test, cnn_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(sentiment_test, cnn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e584172-8997-4b2a-b024-151f5917faa8",
   "metadata": {},
   "source": [
    "### Q2.C - discussion\n",
    "\n",
    "**Performance Summary**\n",
    "\n",
    "\n",
    "| Metric            | RNN (BiLSTM) | TextCNN |\n",
    "|------------------|--------------|---------|\n",
    "| **Training Time** | 62.13 s      | 11.42 s |\n",
    "| **Accuracy**      | 0.8370       | 0.8227  |\n",
    "| **Precision**     | 0.8920       | 0.8588  |\n",
    "| **Recall**        | 0.8152       | 0.8276  |\n",
    "| **F1-score**      | 0.8519       | 0.8429  |\n",
    "\n",
    "It seems that RNN model is slightly accurate compared to TextCNN model due to his high presicion. However, its training time is way longer and has a slightly lower recall value.\n",
    "\n",
    "**Interpretations of the Results**\n",
    "\n",
    "- The Bidirectional LSTM captures sequential information from both the left and right directions. This helps the model understand word ordering, resulting in a higher accuracy.\n",
    "    \n",
    "- From the confusion matrix and the metric table, we can infer that even though RNN is successful at predicting positive sentiments, it can miss some positive sentiments (since it has high precision but slightly lower recall).\n",
    "\n",
    "- However, RNN model's training time is significantly higher than the CNN model. This is due to the fact that standard RNN processing is sequential, where the hidden state $h_t$ depends on $h_{t-1}$. This sequential dependency makes parallelization difficult, resulting in the longer training time.\n",
    "\n",
    "- Whereas the Convolutional layer computes features for all windows across the sentence simultaneously using tensor operations which makes the training highly parallelizable.\n",
    "\n",
    "- CNN model achieved higher recall than the RNN, meaning it detected more true positives, but at the cost of more false positives.\n",
    "\n",
    "**Impact of Design Choices**\n",
    "\n",
    "- During training, I tried to keep the conditions for both model as close as possible since we are not interested in model parameters but the actual model choice.\n",
    "\n",
    "- Randomly initialized Embedding Layer:\n",
    "    - Both models learn task-specific word embeddings during training.\n",
    "    - This is suitable given the dataset size.\n",
    "\n",
    "- Sequence Padding (maxlen = 100):\n",
    "    - Required because batching in TensorFlow needs fixed-length inputs.\n",
    "    - 100 tokens reasonably covers all words.\n",
    "    - Longer reviews are truncated; shorter reviews padded. For some reviews, this resulted in loss of meaning (due to truncation) and meaningless sentences (due to extreme padding).\n",
    "\n",
    "- Bidirectional LSTM:\n",
    "    - Learns context in both directions -> higher accuracy and precision.\n",
    "    - Doubling the computation -> much slower training.\n",
    "\n",
    "- CNN with Conv1D + GlobalMaxPooling:\n",
    "    - Learns n-gram patterns -> good recall.\n",
    "    - Loses ordering information after pooling -> slightly worse precision.\n",
    "    - Extremely fast training due to parallelism.\n",
    "\n",
    "**Final Comparison of Models**\n",
    "\n",
    "                     RNN (BiLSTM)             TextCNN\n",
    "    ---------------------------------------------------------------\n",
    "    Captures word order?     Yes                    No\n",
    "    Captures long-range?     Yes                    No\n",
    "    Detects local phrases?   Moderate               Excellent\n",
    "    Training speed           Slow (62s)             Fast (11s)\n",
    "    Precision                Higher                 Lower\n",
    "    Recall                   Slightly lower         Higher\n",
    "    Best for                 Complex text           Short/simple reviews\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "The RNN achieved the best overall performance (highest accuracy, precision, and F1-score) because it captures detailed context and long-distance dependencies in reviews.\n",
    "\n",
    "The TextCNN, while slightly less accurate, trained nearly six times faster and still produced strong results. Its high recall indicates it effectively identifies positive sentiment when clear cues exist.\n",
    "\n",
    "\n",
    "• Choose RNN when accuracy and interpretability matter.\n",
    "• Choose CNN when speed and scalability are more important.\n",
    "\n",
    "Both models behaved as expected based on their architecture, and both represent \n",
    "valid approaches for sentiment classification on this dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b9930-013d-49e6-88fc-5e20c693fca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
