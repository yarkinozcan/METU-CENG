{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c79918-eec4-4e96-99ab-673c15287380",
   "metadata": {},
   "source": [
    "# CENG463 PA2\n",
    "\n",
    "In this programming assignment, you will be dealing with word embeddings and neural networks. You will use Python for this task. You can use libraries such as `pandas`, `nltk`, `numpy` etc. for your implementations, or implement your own functions. However, you are expected to analyse and reason about your implementation and results. The assignment consists of 3 questions.\n",
    "\n",
    "### IMPORTANT NOTE\n",
    "\n",
    "Do not move or delete the given cells, only add cells inbetween the questions for your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d352749-cf05-4610-b0c1-f6670d538f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tensorflow-macos\n",
      "  Downloading tensorflow_macos-2.16.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow==2.16.2 (from tensorflow-macos)\n",
      "  Downloading tensorflow-2.16.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (18.1.1)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.2->tensorflow-macos)\n",
      "  Downloading ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (23.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.16.2->tensorflow-macos)\n",
      "  Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.76.0)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.2->tensorflow-macos)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.12.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2->tensorflow-macos) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.0)\n",
      "Downloading tensorflow_macos-2.16.2-cp312-cp312-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Downloading tensorflow-2.16.2-cp312-cp312-macosx_12_0_arm64.whl (227.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:19\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl (393 kB)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, ml-dtypes, tensorboard, tensorflow, tensorflow-macos\n",
      "\u001b[2K  Attempting uninstall: protobuf\n",
      "\u001b[2K    Found existing installation: protobuf 6.33.1\n",
      "\u001b[2K    Uninstalling protobuf-6.33.1:\n",
      "\u001b[2K      Successfully uninstalled protobuf-6.33.1\n",
      "\u001b[2K  Attempting uninstall: ml-dtypes\n",
      "\u001b[2K    Found existing installation: ml_dtypes 0.5.4\n",
      "\u001b[2K    Uninstalling ml_dtypes-0.5.4:\n",
      "\u001b[2K      Successfully uninstalled ml_dtypes-0.5.4\n",
      "\u001b[2K  Attempting uninstall: tensorboard\n",
      "\u001b[2K    Found existing installation: tensorboard 2.20.0\n",
      "\u001b[2K    Uninstalling tensorboard-2.20.0:\n",
      "\u001b[2K      Successfully uninstalled tensorboard-2.20.0\n",
      "\u001b[2K  Attempting uninstall: tensorflow━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [tensorboard]\n",
      "\u001b[2K    Found existing installation: tensorflow 2.20.0[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [tensorboard]\n",
      "\u001b[2K    Uninstalling tensorflow-2.20.0:━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [tensorflow]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-2.20.0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [tensorflow-macos][32m3/5\u001b[0m [tensorflow]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ml-dtypes-0.3.2 protobuf-4.25.8 tensorboard-2.16.2 tensorflow-2.16.2 tensorflow-macos-2.16.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tensorflow-metal\n",
      "  Downloading tensorflow_metal-1.2.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-metal) (0.43.0)\n",
      "Requirement already satisfied: six>=1.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-metal) (1.16.0)\n",
      "Downloading tensorflow_metal-1.2.0-cp312-cp312-macosx_12_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorflow-metal\n",
      "Successfully installed tensorflow-metal-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# UPDATE THIS CELL TO INSTALL NEEDED LIBRARIES.\n",
    "# MAKE SURE TO ADD EVERYTHING THAT NEEDS TO BE INSTALLED IN THIS CELL!\n",
    "\n",
    "# we will use pip to install packages - you can add others below\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install --upgrade gensim\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow-macos\n",
    "!pip install tensorflow-metal\n",
    "\n",
    "# and import them here - you can add others below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, Bidirectional,\n",
    "    Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff1907-973f-4895-bd43-0a30c7e0a029",
   "metadata": {},
   "source": [
    "## Q1 - Word embeddings (50 points)\n",
    "\n",
    "In this question, you will first train a Word2Vec model, then use it to represent and reason about user reviews.\n",
    "\n",
    "### Q1.A - training (10 points)\n",
    "Load the `user_review_train.csv` file shared with you. Using `Word2Vec` module of `gensim.models`, train a **skip-gram** Word2Vec model on the train data.\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- Use the given preprocessing function `preprocess_review`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980ad5b5-de0a-4ba0-ad1d-5a45a9543d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yarkinozcan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yarkinozcan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yarkinozcan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING FUNCTIONS GIVEN FOR YOU\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def preprocess_review(review):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = sent_tokenize(review)\n",
    "    \n",
    "    lemmatized_review = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        lowercased_sentence = [token.lower() for token in tokenized_sentence]\n",
    "        stopwords_removed_sentence = [token for token in lowercased_sentence if token not in stop_words]\n",
    "        lemmatized_sentence = [lemmatizer.lemmatize(token) for token in stopwords_removed_sentence]\n",
    "        \n",
    "        lemmatized_review = lemmatized_review + lemmatized_sentence\n",
    "    \n",
    "    return lemmatized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73788893-4f65-4821-a34c-57ac85e4698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized reviews: 14671\n",
      "Model training completed!\n",
      "Model saved as word2vec_skipgram.model\n",
      "Vector for 'good':\n",
      "[-0.23602006  0.24360152 -0.07802209 -0.1251965   0.04575636 -0.3528469\n",
      "  0.18785103  0.65172344 -0.14485672 -0.40459403 -0.00607558 -0.12559274\n",
      " -0.1475443   0.09000729  0.3747157  -0.14832015  0.28471318  0.09323986\n",
      " -0.5221527  -0.40019146 -0.05742176  0.24395053  0.27098238 -0.02191029\n",
      " -0.21682064  0.27232128 -0.12760954  0.10146224  0.10668667  0.00525942\n",
      "  0.11954835  0.03860368 -0.13652475 -0.25755656 -0.20633948  0.33619738\n",
      "  0.13563193  0.00927034 -0.25880328 -0.18385689 -0.02204607  0.15687074\n",
      "  0.00668585 -0.03372012  0.39808056 -0.25141117 -0.17338398 -0.1351257\n",
      "  0.09212077  0.07840645  0.04531801 -0.23774526  0.14126235 -0.04810696\n",
      " -0.03855121 -0.20607518  0.3029945  -0.16748565 -0.18449892  0.13847631\n",
      "  0.20515808 -0.25355488  0.33550352 -0.00530628 -0.18030615  0.2553994\n",
      "  0.36877277  0.24912655 -0.71450436  0.47260785  0.14221273  0.15705653\n",
      "  0.18210343  0.15601468  0.32270834  0.18767521 -0.08921938  0.21751295\n",
      " -0.16763878 -0.16660924  0.01929437  0.0154322  -0.3195129   0.17641564\n",
      " -0.09745414 -0.14946395  0.5566759   0.08944939 -0.13371596  0.04354782\n",
      "  0.2503145   0.26065406  0.23142497 -0.07760198  0.7168734   0.11332039\n",
      "  0.01758514  0.11168845  0.17376597 -0.10281438]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the training data\n",
    "train_df = pd.read_csv(\"data/user_review_train.csv\")   # adjust if needed\n",
    "\n",
    "# 2. Preprocess every review using the provided function\n",
    "tokenized_reviews = []\n",
    "\n",
    "for review in train_df[\"review\"]:\n",
    "    if isinstance(review, str):\n",
    "        tokens = preprocess_review(review)   # uses your preprocessing function\n",
    "        if len(tokens) > 0:\n",
    "            tokenized_reviews.append(tokens)\n",
    "\n",
    "print(\"Number of tokenized reviews:\", len(tokenized_reviews))\n",
    "\n",
    "# 3. Train the skip-gram Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_reviews,\n",
    "    vector_size=100,          # embedding dimension (change if needed)\n",
    "    window=5,                 # context window\n",
    "    min_count=3,              # ignore rare words\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    sg=1,                     # 1 = skip-gram (as required)\n",
    "    negative=5,               # negative sampling\n",
    "    epochs=5                  # training epochs\n",
    ")\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "# 4. Save the trained model (optional but recommended)\n",
    "w2v_model.save(\"word2vec_skipgram.model\")\n",
    "print(\"Model saved as word2vec_skipgram.model\")\n",
    "\n",
    "# 5. Quick example test\n",
    "example_word = \"good\"\n",
    "if example_word in w2v_model.wv:\n",
    "    print(\"Vector for 'good':\")\n",
    "    print(w2v_model.wv[example_word])\n",
    "else:\n",
    "    print(\"'good' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fed93-b026-4ccb-931c-b2afc1f9e9ea",
   "metadata": {},
   "source": [
    "### Q1.B - word similarity (10 points)\n",
    "\n",
    "Using the trained model, report the following:\n",
    "\n",
    "- Similarity between \"good\" and \"bad\"\n",
    "- Similar words to \"good\"\n",
    "- Similar words to \"bad\"\n",
    "- Similar words to \"good\" but not similar to \"bad\"\n",
    "- Similar words to \"good\" but not similar to \"bad\"\n",
    "\n",
    "and discuss the reported words and scores. Is it possible to identify specific good/bad features of the product that is being reviewed? What other words can be looked up to get more insight?\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- Check the [documentation](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html) of `gensim.models.Word2Vec` to find relevant methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997de6f5-d160-4a2a-a835-89bd13e5ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between \"good\" and \"bad\": 0.6645\n",
      "\n",
      "Top similar words to \"good\":\n",
      "  nice             0.8896\n",
      "  perfect          0.8851\n",
      "  awesome          0.8694\n",
      "  gud              0.8669\n",
      "  superb           0.8634\n",
      "  except           0.8628\n",
      "  impressive       0.8620\n",
      "  excellent        0.8589\n",
      "  described        0.8567\n",
      "  okay             0.8565\n",
      "\n",
      "Top similar words to \"bad\":\n",
      "  poor             0.8133\n",
      "  worst            0.7557\n",
      "  nd               0.7514\n",
      "  bed              0.7438\n",
      "  3rd              0.7429\n",
      "  satisfactory     0.7413\n",
      "  dull             0.7391\n",
      "  vry              0.7366\n",
      "  bettry           0.7345\n",
      "  pathetic         0.7336\n",
      "\n",
      "Words similar to \"good\" but NOT similar to \"bad\":\n",
      "  ram              0.3879\n",
      "  budget           0.3850\n",
      "  gb               0.3666\n",
      "  except           0.3608\n",
      "  memory           0.3582\n",
      "  internal         0.3545\n",
      "  everything       0.3482\n",
      "  cool             0.3395\n",
      "  slightly         0.3384\n",
      "  spec             0.3345\n",
      "\n",
      "Words similar to \"bad\" but NOT similar to \"good\":\n",
      "  customer         0.3770\n",
      "  care             0.3670\n",
      "  told             0.2842\n",
      "  technician       0.2654\n",
      "  policy           0.2557\n",
      "  person           0.2508\n",
      "  tried            0.2492\n",
      "  contacted        0.2412\n",
      "  amazon           0.2408\n",
      "  called           0.2378\n"
     ]
    }
   ],
   "source": [
    "# 1) Similarity between \"good\" and \"bad\"\n",
    "sim_good_bad = w2v_model.wv.similarity(\"good\", \"bad\")\n",
    "print(f'Similarity between \"good\" and \"bad\": {sim_good_bad:.4f}\\n')\n",
    "\n",
    "# 2) Similar words to \"good\"\n",
    "print('Top similar words to \"good\":')\n",
    "similar_to_good = w2v_model.wv.most_similar(\"good\", topn=10)\n",
    "for word, score in similar_to_good:\n",
    "    print(f\"  {word:15s}  {score:.4f}\")\n",
    "print()\n",
    "\n",
    "# 3) Similar words to \"bad\"\n",
    "print('Top similar words to \"bad\":')\n",
    "similar_to_bad = w2v_model.wv.most_similar(\"bad\", topn=10)\n",
    "for word, score in similar_to_bad:\n",
    "    print(f\"  {word:15s}  {score:.4f}\")\n",
    "print()\n",
    "\n",
    "# 4) Words similar to \"good\" but not similar to \"bad\"\n",
    "print('Words similar to \"good\" but NOT similar to \"bad\":')\n",
    "good_not_bad = w2v_model.wv.most_similar(\n",
    "    positive=[\"good\"],\n",
    "    negative=[\"bad\"],\n",
    "    topn=10\n",
    ")\n",
    "for word, score in good_not_bad:\n",
    "    print(f\"  {word:15s}  {score:.4f}\")\n",
    "print()\n",
    "\n",
    "# 5) Words similar to \"bad\" but NOT similar to \"good\"\n",
    "print('Words similar to \"bad\" but NOT similar to \"good\":')\n",
    "bad_not_good = w2v_model.wv.most_similar(\n",
    "    positive=[\"bad\"],\n",
    "    negative=[\"good\"],\n",
    "    topn=10\n",
    ")\n",
    "for word, score in bad_not_good:\n",
    "    print(f\"  {word:15s}  {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b83305-d952-4d88-bc46-0dadd68f26a9",
   "metadata": {},
   "source": [
    "### Q1.B - discussion\n",
    "Write your discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c25cf0-926a-4536-b2e5-f0bdb1931672",
   "metadata": {},
   "source": [
    "### Q1.C - representation (15 points)\n",
    "\n",
    "An important use of word embeddings is representing \"documents\" (reviews in our case). For this question, before creating the representations, do the following:\n",
    "\n",
    "- Randomly sample 2 reviews from sentiment label 0, refer to them as sent0_a and sent0_b.\n",
    "- Randomly sample 2 reviews from sentiment label 1, refer to them as sent1_a and sent1_b.\n",
    "\n",
    "After the sampling, follow these steps to represent each review:\n",
    "\n",
    "- Preprocess the review with the given `preprocess_review` function.\n",
    "- For each token in the review, fetch the vector of that token.\n",
    "- Take the average of the token vectors in the review to represent that review.\n",
    "\n",
    "Then, calculate and report the cosine similarity of the two vectors representing:\n",
    "    - sent0_a and sent0_b\n",
    "    - sent0_a and sent1_a\n",
    "    - sent1_a and sent1_b\n",
    "\n",
    "Does this representation work to capture the labels of the reviews? Do you think there is a better way to represent each review instead of taking the average of the word vectors? Discuss your findings with respect to these questions. Repeating the sampling process several times might give you a better insight.\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- You can use `numpy` for your calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bf5723-ee84-46a0-b758-ecd0ad8d055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent0_a (label 0):\n",
      " It takes lot of time for charging full battery, more than 6 hours for one full charge and heating problem.. \n",
      "\n",
      "sent0_b (label 0):\n",
      " Bluetooth is not working properly with my speaker getting breaking sound most of the time \n",
      "\n",
      "sent1_a (label 1):\n",
      " Superb \n",
      "\n",
      "sent1_b (label 1):\n",
      " Cons: 1.Heating Issue - Phone easily gets at around 40 degrees while charging and also while playing heavy games or camera.2. Camera - After the software update you may experience depth mode little better, still the edges get blurred.3. Battery Backup - Its written 4000mah but equivalent to 3500mah.4. Slight Slutter, No Lag5. 4 Gb Ram Variant is preferable & for performance as well and also black variant looks cool and premium.6. You cannot put a tempered glass on its display as 2.5D curved edges will show up bubbles around edges.7. They did provide a turbo charger but it takes 2.2 hours for this phone to charge fully. \n",
      "\n",
      "cosine(sent0_a, sent0_b) = 0.7643\n",
      "cosine(sent0_a, sent1_a) = 0.6476\n",
      "cosine(sent1_a, sent1_b) = 0.7255\n"
     ]
    }
   ],
   "source": [
    "# 1) Randomly sample 2 reviews from each sentiment\n",
    "\n",
    "# Sentiment 0 (e.g., negative)\n",
    "sent0_df = train_df[train_df[\"sentiment\"] == 0]\n",
    "sent0_samples = sent0_df.sample(2, random_state=1)  # change random_state to resample\n",
    "\n",
    "# Sentiment 1 (e.g., positive)\n",
    "sent1_df = train_df[train_df[\"sentiment\"] == 1]\n",
    "sent1_samples = sent1_df.sample(2, random_state=1)\n",
    "\n",
    "sent0_a_text = sent0_samples.iloc[0][\"review\"]\n",
    "sent0_b_text = sent0_samples.iloc[1][\"review\"]\n",
    "sent1_a_text = sent1_samples.iloc[0][\"review\"]\n",
    "sent1_b_text = sent1_samples.iloc[1][\"review\"]\n",
    "\n",
    "print(\"sent0_a (label 0):\\n\", sent0_a_text, \"\\n\")\n",
    "print(\"sent0_b (label 0):\\n\", sent0_b_text, \"\\n\")\n",
    "print(\"sent1_a (label 1):\\n\", sent1_a_text, \"\\n\")\n",
    "print(\"sent1_b (label 1):\\n\", sent1_b_text, \"\\n\")\n",
    "\n",
    "\n",
    "# 2) Helper: get average embedding for a review\n",
    "\n",
    "def get_review_vector(review_text, model):\n",
    "    tokens = preprocess_review(review_text)\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        # If no token has a vector (very rare), return a zero vector\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "# 3) Helper: cosine similarity between two vectors\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(v1, v2) / (norm1 * norm2))\n",
    "\n",
    "\n",
    "# 4) Compute review vectors\n",
    "\n",
    "sent0_a_vec = get_review_vector(sent0_a_text, w2v_model)\n",
    "sent0_b_vec = get_review_vector(sent0_b_text, w2v_model)\n",
    "sent1_a_vec = get_review_vector(sent1_a_text, w2v_model)\n",
    "sent1_b_vec = get_review_vector(sent1_b_text, w2v_model)\n",
    "\n",
    "\n",
    "# 5) Compute requested cosine similarities\n",
    "\n",
    "sim_00 = cosine_similarity(sent0_a_vec, sent0_b_vec)  # 0 vs 0\n",
    "sim_01 = cosine_similarity(sent0_a_vec, sent1_a_vec)  # 0 vs 1\n",
    "sim_11 = cosine_similarity(sent1_a_vec, sent1_b_vec)  # 1 vs 1\n",
    "\n",
    "print(f\"cosine(sent0_a, sent0_b) = {sim_00:.4f}\")\n",
    "print(f\"cosine(sent0_a, sent1_a) = {sim_01:.4f}\")\n",
    "print(f\"cosine(sent1_a, sent1_b) = {sim_11:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed704b27-9a9e-4af1-8f98-6ee4aeaeab4e",
   "metadata": {},
   "source": [
    "### Q1.C - discussion\n",
    "Write your discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb65f33-6f79-44e8-8b68-c054ad7a7707",
   "metadata": {},
   "source": [
    "### Q1.D - training and comparing classifiers (15 points)\n",
    "\n",
    "For this task, you will use the `user_review_train.csv` and `user_review_test.csv` files to train a binary classification model with Word2Vec representations, and compare its performance with a binary classifier using Bag-of-Words representation.\n",
    "\n",
    "As the Bag-of-Words classifier, you can either choose the best performing classifier you have implemented in Question 3 of Programming Assignment 1, or you can follow these steps:\n",
    "\n",
    "- Preprocess the review with the given `preprocess_review` function.\n",
    "- Order all unique tokens by frequency, take the most frequent 100.\n",
    "- Use these 100 words as the corpus for Bag-of-Words representation.\n",
    "\n",
    "For the Word2Vec model, represent the reviews by following these steps:\n",
    "\n",
    "- Preprocess the review with the given `preprocess_review` function.\n",
    "- For each token in the review that is also in the most frequent 100 tokens, fetch the vector of that token.\n",
    "- Take the average of the token vectors selected to represent that review.\n",
    "\n",
    "After training both classifiers on `user_review_train.csv`, test them with `user_review_test.csv` and report the performance of your models with four metrics: accuracy, precision, recall and F1-score. Compare the performance of both models and discuss in detail.\n",
    "\n",
    "#### Notes and tips\n",
    "\n",
    "- You can use `CountVectorizer` from `scikit-learn` or any other library available for Bag-of-Words representation.\n",
    "- You should select a classification method from the following set of classifiers: `[Naive Bayes, Support Vector Machine, Logistic Regression, Random Forest]`. You can use `scikit-learn`, `nltk`, or any other library for the classifier implementations. \n",
    "- You should **not** use the test set `user_reviews_test.csv` during your training process. You should use `user_reviews_train.csv` only.\n",
    "- You may add a validation step in your training process. To do this, you can further split the `user_reviews_train.csv` data and apply k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe260954-3312-4f85-9834-ee74d5988e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 of the 100 most frequent tokens: ['.', 'phone', ',', 'good', 'camera', 'battery', 'mobile', '...', '..', 'product']\n",
      "Vocabulary size: 100\n",
      "BoW feature matrix shapes: (14675, 100) (1675, 100)\n",
      "Word2Vec feature matrix shapes: (14675, 100) (1675, 100)\n",
      "=== Bag-of-Words + LogisticRegression ===\n",
      "Accuracy : 0.8119\n",
      "Precision: 0.8313\n",
      "Recall   : 0.8442\n",
      "F1-score : 0.8377\n",
      "\n",
      "=== Word2Vec-avg (top-100) + LogisticRegression ===\n",
      "Accuracy : 0.7904\n",
      "Precision: 0.8408\n",
      "Recall   : 0.7840\n",
      "F1-score : 0.8114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Load train and test data\n",
    "train_df = pd.read_csv(\"data/user_review_train.csv\")\n",
    "test_df  = pd.read_csv(\"data/user_review_test.csv\")\n",
    "\n",
    "# Make sure we have 'review' and 'sentiment' columns\n",
    "y_train = train_df[\"sentiment\"].values\n",
    "y_test  = test_df[\"sentiment\"].values\n",
    "\n",
    "# 2) Preprocess all reviews using the given function\n",
    "\n",
    "train_tokens_list = [preprocess_review(str(r)) for r in train_df[\"review\"]]\n",
    "test_tokens_list  = [preprocess_review(str(r)) for r in test_df[\"review\"]]\n",
    "\n",
    "# 3) Build vocabulary: most frequent 100 tokens in the TRAIN set\n",
    "\n",
    "freq = Counter()\n",
    "for tokens in train_tokens_list:\n",
    "    freq.update(tokens)\n",
    "\n",
    "most_common_100 = [w for w, c in freq.most_common(100)]\n",
    "vocab_index = {w: i for i, w in enumerate(most_common_100)}\n",
    "vocab_set = set(most_common_100)\n",
    "\n",
    "print(\"Top 10 of the 100 most frequent tokens:\", most_common_100[:10])\n",
    "print(\"Vocabulary size:\", len(most_common_100))\n",
    "\n",
    "\n",
    "# 4) Bag-of-Words representation (counts of top-100 tokens)\n",
    "\n",
    "def bow_from_tokens(tokens, vocab_index):\n",
    "    vec = np.zeros(len(vocab_index), dtype=np.float32)\n",
    "    for t in tokens:\n",
    "        if t in vocab_index:\n",
    "            vec[vocab_index[t]] += 1.0\n",
    "    return vec\n",
    "\n",
    "X_train_bow = np.vstack([bow_from_tokens(tokens, vocab_index) for tokens in train_tokens_list])\n",
    "X_test_bow  = np.vstack([bow_from_tokens(tokens, vocab_index) for tokens in test_tokens_list])\n",
    "\n",
    "print(\"BoW feature matrix shapes:\", X_train_bow.shape, X_test_bow.shape)\n",
    "\n",
    "\n",
    "# 5) Word2Vec-based representation (average of W2V vectors for tokens in top-100)\n",
    "\n",
    "def w2v_review_vector(tokens, model, allowed_tokens):\n",
    "    vecs = []\n",
    "    for t in tokens:\n",
    "        if t in allowed_tokens and t in model.wv:\n",
    "            vecs.append(model.wv[t])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "X_train_w2v = np.vstack([\n",
    "    w2v_review_vector(tokens, w2v_model, vocab_set) for tokens in train_tokens_list\n",
    "])\n",
    "X_test_w2v = np.vstack([\n",
    "    w2v_review_vector(tokens, w2v_model, vocab_set) for tokens in test_tokens_list\n",
    "])\n",
    "\n",
    "print(\"Word2Vec feature matrix shapes:\", X_train_w2v.shape, X_test_w2v.shape)\n",
    "\n",
    "\n",
    "# 6) Train classifiers (Logistic Regression for both)\n",
    "\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train_bow, y_train)\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)\n",
    "\n",
    "clf_w2v = LogisticRegression(max_iter=1000)\n",
    "clf_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_w2v = clf_w2v.predict(X_test_w2v)\n",
    "\n",
    "\n",
    "# 7) Compute evaluation metrics\n",
    "\n",
    "def evaluate_model(y_true, y_pred, name=\"model\"):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec  = recall_score(y_true, y_pred)\n",
    "    f1   = f1_score(y_true, y_pred)\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\")\n",
    "    print()\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "metrics_bow  = evaluate_model(y_test, y_pred_bow,  name=\"Bag-of-Words + LogisticRegression\")\n",
    "metrics_w2v  = evaluate_model(y_test, y_pred_w2v,  name=\"Word2Vec-avg (top-100) + LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5472c-1f1e-4b78-bd3e-0a9014c29337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.A - RNN (Bidirectional LSTM) MODEL\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "rnn_model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen\n",
    "    ),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "rnn_model.summary()\n",
    "\n",
    "# Train and measure time\n",
    "start_time = time.time()\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "end_time = time.time()\n",
    "rnn_train_time = end_time - start_time\n",
    "print(\"RNN training time (seconds):\", rnn_train_time)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rnn_proba = rnn_model.predict(X_test_pad).reshape(-1)\n",
    "y_pred_rnn = (y_pred_rnn_proba >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "cm_rnn = confusion_matrix(y_test, y_pred_rnn)\n",
    "acc_rnn = accuracy_score(y_test, y_pred_rnn)\n",
    "prec_rnn = precision_score(y_test, y_pred_rnn)\n",
    "rec_rnn = recall_score(y_test, y_pred_rnn)\n",
    "f1_rnn = f1_score(y_test, y_pred_rnn)\n",
    "auc_rnn = roc_auc_score(y_test, y_pred_rnn_proba)\n",
    "\n",
    "print(\"=== RNN Confusion Matrix ===\")\n",
    "print(cm_rnn)\n",
    "print(f\"RNN Accuracy : {acc_rnn:.4f}\")\n",
    "print(f\"RNN Precision: {prec_rnn:.4f}\")\n",
    "print(f\"RNN Recall   : {rec_rnn:.4f}\")\n",
    "print(f\"RNN F1-score : {f1_rnn:.4f}\")\n",
    "print(f\"RNN AUC      : {auc_rnn:.4f}\")\n",
    "print(f\"RNN Training time (s): {rnn_train_time:.2f}\")\n",
    "\n",
    "# Q2.A - RNN (Bidirectional LSTM) MODEL\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "rnn_model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen\n",
    "    ),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "rnn_model.summary()\n",
    "\n",
    "# Train and measure time\n",
    "start_time = time.time()\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "end_time = time.time()\n",
    "rnn_train_time = end_time - start_time\n",
    "print(\"RNN training time (seconds):\", rnn_train_time)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rnn_proba = rnn_model.predict(X_test_pad).reshape(-1)\n",
    "y_pred_rnn = (y_pred_rnn_proba >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "cm_rnn = confusion_matrix(y_test, y_pred_rnn)\n",
    "acc_rnn = accuracy_score(y_test, y_pred_rnn)\n",
    "prec_rnn = precision_score(y_test, y_pred_rnn)\n",
    "rec_rnn = recall_score(y_test, y_pred_rnn)\n",
    "f1_rnn = f1_score(y_test, y_pred_rnn)\n",
    "auc_rnn = roc_auc_score(y_test, y_pred_rnn_proba)\n",
    "\n",
    "print(\"=== RNN Confusion Matrix ===\")\n",
    "print(cm_rnn)\n",
    "print(f\"RNN Accuracy : {acc_rnn:.4f}\")\n",
    "print(f\"RNN Precision: {prec_rnn:.4f}\")\n",
    "print(f\"RNN Recall   : {rec_rnn:.4f}\")\n",
    "print(f\"RNN F1-score : {f1_rnn:.4f}\")\n",
    "print(f\"RNN AUC      : {auc_rnn:.4f}\")\n",
    "print(f\"RNN Training time (s): {rnn_train_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7b939-e486-4fa1-b053-53f2e851992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.B - implementation of TextCNN\n",
    "# you can add cells below if needed\n",
    "\n",
    "# Q2.B - TextCNN MODEL\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=maxlen\n",
    "    ),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train and measure time\n",
    "start_time = time.time()\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "end_time = time.time()\n",
    "cnn_train_time = end_time - start_time\n",
    "print(\"TextCNN training time (seconds):\", cnn_train_time)\n",
    "\n",
    "# Predictions\n",
    "y_pred_cnn_proba = cnn_model.predict(X_test_pad).reshape(-1)\n",
    "y_pred_cnn = (y_pred_cnn_proba >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "cm_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
    "acc_cnn = accuracy_score(y_test, y_pred_cnn)\n",
    "prec_cnn = precision_score(y_test, y_pred_cnn)\n",
    "rec_cnn = recall_score(y_test, y_pred_cnn)\n",
    "f1_cnn = f1_score(y_test, y_pred_cnn)\n",
    "auc_cnn = roc_auc_score(y_test, y_pred_cnn_proba)\n",
    "\n",
    "print(\"=== TextCNN Confusion Matrix ===\")\n",
    "print(cm_cnn)\n",
    "print(f\"TextCNN Accuracy : {acc_cnn:.4f}\")\n",
    "print(f\"TextCNN Precision: {prec_cnn:.4f}\")\n",
    "print(f\"TextCNN Recall   : {rec_cnn:.4f}\")\n",
    "print(f\"TextCNN F1-score : {f1_cnn:.4f}\")\n",
    "print(f\"TextCNN AUC      : {auc_cnn:.4f}\")\n",
    "print(f\"TextCNN Training time (s): {cnn_train_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e584172-8997-4b2a-b024-151f5917faa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Q2.C - discussion\n",
    "\n",
    "Write your discussion here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
